// Licensed to Cloudera, Inc. under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  Cloudera, Inc. licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
{
  "name": "KAFKA",
  "label": "Kafka",
  "description": "Apache Kafka is publish-subscribe messaging rethought as a distributed commit log. <span class=\"error\">Before adding this service, ensure that either the Kafka parcel is activated or the Kafka package is installed.</span>",
  "version": "1.4.0",
  "compatibility": {
    "generation": 1,
    "cdhVersion": {
      "min": "5",
      "max": "5"
    }
  },
  "runAs": {
    "user": "kafka",
    "group": "kafka"
  },
  "icon": "images/kafka.png",
  "parcel": {
    "repoUrl": "https://archive.cloudera.com/kafka/parcels/latest/",
    "requiredTags": [
      "cdh",
      "kafka"
    ]
  },
  "serviceDependencies": [
    {
      "name": "ZOOKEEPER",
      "required": "true"
    },
    {
      "name": "SENTRY",
      "required": "false"
    }
  ],
  "inExpressWizard": true,
  "kerberos" : "${kerberos.auth.enable}",
  "parameters": [
    {
      "name": "zookeeper.chroot",
      "label": "ZooKeeper Root",
      "description": "ZNode in ZooKeeper that should be used as a root for this Kafka cluster.",
      "type": "string",
      "default": "",
      "configurableInWizard": true
    },
    {
      "name" : "kerberos.auth.enable",
      "label" : "Enable Kerberos Authentication",
      "description" : "Enable Kerberos authentication for this KAFKA service.",
      "type" : "boolean",
      "default" : "false",
      "configurableInWizard": true
    },
    {
      "name": "auto.create.topics.enable",
      "label": "Topic Auto Creation",
      "description": "Enables auto creation of topics on the server. If this is set to true, then attempts to produce, consume, or fetch metadata for a non-existent topic automatically create the topic with the default replication factor and number of partitions.",
      "type": "boolean",
      "default": "true",
      "configurableInWizard": true
    },
    {
      "name": "num.partitions",
      "label": "Default Number of Partitions",
      "description": "The default number of partitions for automatically created topics.",
      "type": "long",
      "default": 1,
      "min": 1,
      "max": 2147483647,
      "softMax": 1000
    },
    {
      "name": "default.replication.factor",
      "label": "Default Replication Factor",
      "description": "The default replication factor for automatically created topics.",
      "type": "long",
      "default": 1,
      "min": 1,
      "max": 2147483647,
      "softMax": 10,
      "configurableInWizard": true
    },
    {
      "name": "min.insync.replicas",
      "label": "Minimum Number of Replicas in ISR",
      "description": "The minimum number of replicas in the in-sync replica needed to satisfy a produce request where required.acks=-1 (that is, all).",
      "type": "long",
      "default": 1,
      "min": 1,
      "max": 2147483647,
      "softMax": 10
    },
    {
      "name": "offsets.topic.num.partitions",
      "label": "Offset Commit Topic Number of Partitions",
      "description": "The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, Cloudera recommends using a higher setting for production (for example, 100-200).",
      "type": "long",
      "default": 50,
      "min": 1,
      "max": 2147483647,
      "softMin": 25,
      "softMax": 1000,
      "configurableInWizard": true
    },
    {
      "name": "offsets.topic.replication.factor",
      "label": "Offset Commit Topic Replication Factor",
      "description": "The replication factor for the offset commit topic. A higher setting is recommended in order to ensure higher availability (for example, 3 or 4) . If the offsets topic is created when there are fewer brokers than the replication factor, then the offsets topic is created with fewer replicas.",
      "type": "long",
      "default": 3,
      "min": 1,
      "max": 2147483647,
      "softMin": 2,
      "softMax": 10,
      "configurableInWizard": true
    },
    {
      "name": "delete.topic.enable",
      "label": "Enable Delete Topic",
      "description": "Enables topic deletion using admin tools. When delete topic is disabled, deleting topics through the admin tools has no effect.",
      "type": "boolean",
      "default": true
    },
    {
      "name": "controlled.shutdown.enable",
      "label": "Enable Controlled Shutdown",
      "description": "Enables controlled shutdown of the broker. If enabled, the broker moves all leaders on it to other brokers before shutting itself down. This reduces the unavailability window during shutdown.",
      "type": "boolean",
      "default": true
    },
    {
      "name": "controlled.shutdown.max.retries",
      "label": "Controlled Shutdown Maximum Attempts",
      "description": "Number of unsuccessful controlled shutdown attempts before executing an unclean shutdown. For example, the default value of 3 means that the system will attempt a controlled shutdown 3 times before executing an unclean shutdown.",
      "type": "long",
      "default": 3,
      "min": 0,
      "max": 2147483647,
      "softMax": 10
    },
    {
      "name": "unclean.leader.election.enable",
      "label": "Enable Unclean Leader Election",
      "description": "Enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so might result in data loss.",
      "type": "boolean",
      "default": false
    },
    {
      "name": "auto.leader.rebalance.enable",
      "label": "Enable Automatic Leader Rebalancing",
      "description": "If automatic leader rebalancing is enabled, the controller tries to balance leadership for partitions among the brokers by periodically returning leadership for each partition to the preferred replica, if it is available.",
      "type": "boolean",
      "default": true
    },
    {
      "name": "leader.imbalance.per.broker.percentage",
      "label": "Leader Imbalance Allowed Per Broker",
      "description": "The percentage of leader imbalance allowed per broker. The controller rebalances leadership if this ratio goes above the configured value per broker.",
      "type": "long",
      "default": 10,
      "min": 1,
      "max": 100,
      "softMin": 5,
      "softMax": 50,
      "unit": "percent"
    },
    {
      "name": "leader.imbalance.check.interval.seconds",
      "label": "Leader Imbalance Check Interval",
      "description": "The frequency with which to check for leader imbalance.",
      "type": "long",
      "default": 300,
      "min": 1,
      "max": 2147483647,
      "softMin": 30,
      "softMax": 604800,
      "unit": "seconds"
    },
    {
      "name": "zookeeper.session.timeout.ms",
      "label": "ZooKeeper Session Timeout",
      "description": "If the server fails to send a heartbeat to ZooKeeper within this period of time, it is considered dead. If set too low, ZooKeeper might falsely consider a server dead; if set too high, ZooKeeper might take too long to recognize a dead server.",
      "type": "long",
      "default": 6000,
      "min": 1,
      "max": 2147483647,
      "softMax": 3600000,
      "unit": "milliseconds"
    },
    {
      "name": "message.max.bytes",
      "label": "Maximum Message Size",
      "description": "The maximum size of a message that the server can receive. It is important that this property be in sync with the maximum fetch size the consumers use, or else an unruly producer could publish messages too large for consumers to consume.",
      "type": "long",
      "default": 1000000,
      "max": 2147483647,
      "min": 1,
      "softMin": 1024,
      "softMax": 10485760,
      "unit": "bytes"
    },
    {
      "name": "replica.fetch.max.bytes",
      "label": "Replica Maximum Fetch Size",
      "description": "The maximum number of bytes to fetch for each partition in fetch requests replicas send to the leader. This value should be larger than message.max.bytes.",
      "type": "long",
      "default": 1048576,
      "min": 1,
      "max": 2147483647,
      "softMin": 1024,
      "softMax": 1073741824,
      "unit": "bytes"
    },
    {
      "name": "num.replica.fetchers",
      "label": "Number of Replica Fetchers",
      "description": "Number of threads used to replicate messages from leaders. Increasing this value increases the degree of I/O parallelism in the follower broker.",
      "type": "long",
      "default": 1,
      "min": 1,
      "max": 2147483647,
      "softMax": 100
    },
    {
      "name": "replica.lag.max.messages",
      "label": "Allowed Replica Message Lag",
      "description": "If a replica falls more than this number of messages behind the leader, the leader removes the follower from the ISR and treats it as dead. This property is deprecated in Kafka 1.4.0; higher versions use only replica.lag.time.max.ms.",
      "type": "long",
      "default": 4000,
      "min": 1,
      "max": 2147483647
    },
    {
      "name": "replica.lag.time.max.ms",
      "label": "Allowed Replica Time Lag",
      "description": "If a follower has not sent any fetch requests, nor has it consumed up to the leader's log end offset during this time, the leader removes the follower from the ISR set.",
      "type": "long",
      "default": 10000,
      "min": 1,
      "unit": "milliseconds"
    },
    {
      "name": "log.flush.interval.messages",
      "label": "Log Flush Message Interval",
      "description": "The number of messages written to a log partition before triggering an fsync on the log. Setting this lower syncs data to disk more often, but has a major impact on performance. Cloudera recommends use of replication for durability rather than depending on single-server fsync; however, this setting can be used to be extra certain. If used in conjunction with log.flush.interval.ms, the log is flushed when either criteria is met.",
      "type": "long",
      "min": 1
    },
    {
      "name": "log.flush.interval.ms",
      "label": "Log Flush Time Interval",
      "description": "The maximum time between fsync calls on the log. If used in conjuction with log.flush.interval.messages, the log is flushed when either criteria is met.",
      "type": "long",
      "min": 1,
      "unit": "milliseconds"
    },
    {
      "name": "log.flush.scheduler.interval.ms",
      "label": "Log Flush Scheduler Interval",
      "description": "The frequency, in ms, with which the log flusher checks whether any log is eligible to be flushed to disk.",
      "type": "long",
      "min": 1,
      "unit": "milliseconds"
    },
    {
      "name": "log.cleaner.enable",
      "label": "Enable Log Compaction",
      "description": "Enables the log cleaner to compact topics with cleanup.policy=compact on this cluster.",
      "type": "boolean",
      "default": true
    },
    {
      "name": "log.cleaner.threads",
      "label": "Number of Log Cleaner Threads",
      "description": "The number of background threads to use for log cleaning.",
      "type": "long",
      "default": 1,
      "min": 1,
      "max": 2147483647
    },
    {
      "name": "log.cleaner.dedupe.buffer.size",
      "label": "Log Cleaner Deduplication Buffer Size",
      "description": "The total memory used for log deduplication across all cleaner threads. This memory is statically allocated and will not cause GC problems.",
      "type": "long",
      "default": 134217728,
      "min": 1048576,
      "max": 2147483647,
      "unit": "bytes"
    },
    {
      "name": "log.cleaner.min.cleanable.ratio",
      "label": "Log Cleaner Clean Ratio",
      "description": "Controls how frequently the log cleaner will attempt to clean the log. This ratio bounds the maximum space wasted in the log by duplicates. For example, at 0.5 at most 50% of the log could be duplicates. A higher ratio will mean fewer, more efficient cleanings but will mean more wasted space in the log.",
      "type": "double",
      "default": 0.50,
      "min": 0,
      "max": 1
    },
    {
      "name": "log.cleaner.delete.retention.ms",
      "label": "Log Compaction Delete Record Retention Time",
      "description": "The amount of time to retain delete messages for log compacted topics. Once a consumer has seen an original message you need to ensure it also sees the delete message. If you removed the delete message too quickly, this might not happen. As a result there is a configurable delete retention time.",
      "type": "long",
      "default": 604800000,
      "min": -1,
      "unit": "milliseconds"
    },
    {
      "name": "quota.consumer.default",
      "label": "Default Consumer Quota",
      "description": "Any consumer distinguished by clientId/consumer group will get throttled if it fetches more bytes than this value per-second. Only respected by Kafka 2.0 or later.",
      "type": "long",
      "min": 0,
      "unit": "bytes"
    },
    {
      "name": "quota.producer.default",
      "label": "Default Producer Quota",
      "description": "Any producer distinguished by clientId will get throttled if it produces more bytes than this value per-second. Only respected by Kafka 2.0 or later.",
      "type": "long",
      "min": 0,
      "unit": "bytes"
    },
    {
      "name": "monitoring.enabled",
      "label": "Enable Kafka Monitoring (Note: Requires Kafka-1.3.0 parcel or higher)",
      "description": "Enables Kafka monitoring.",
      "type": "boolean",
      "default": true,
      "configurableInWizard": true
    },
    {
      "name": "kafka.metrics.reporters",
      "label": "List of Metric Reporters",
      "description": "List of metric reporter class names. HTTP reporter is included by default.",
      "type": "string_array",
      "default": [
        "nl.techop.kafka.KafkaHttpMetricsReporter"
      ],
      "minLength": 1
    },
    {
      "name": "sentry.kafka.caching.enable",
      "label": "Enable Sentry Privileges Caching",
      "description": "Enables Sentry privileges caching. Only applicable if authorization via Sentry is enabled.",
      "type": "boolean",
      "default": true
    },
    {
      "name": "sentry.kafka.caching.ttl.ms",
      "label": "Sentry Privileges Cache Refresh Interval",
      "description": "The amount of time between updates of the Sentry privileges cache. Any change in role or privilege can take up to the specified time for changes to reflect in the cache.",
      "type": "long",
      "default": 30000,
      "min": 1,
      "unit": "milliseconds"
    },
    {
      "name": "sentry.kafka.caching.update.failures.count",
      "label": "Sentry Privileges Cache Maximum Update Failures",
      "description": "If Sentry privileges cache fails to update for specified number of times, all authorization requests will be denied.",
      "type": "long",
      "default": 3,
      "min": 1,
      "max": 2147483647
    }
  ],
  "roles": [
    {
      "name": "KAFKA_BROKER",
      "label": "Kafka Broker",
      "pluralLabel": "Kafka Brokers",
      "jvmBased": true,
      "uniqueIdParameters": [
        "broker.id"
      ],
      "startRunner": {
        "program": "scripts/control.sh",
        "args": [
          "start"
        ],
        "environmentVariables": {
          "HOST": "${host}",
          "CHROOT": "${zookeeper.chroot}",
          "JMX_PORT": "${jmx_port}",
          "PORT": "${port}",
          "SSL_PORT": "${ssl_port}",
          "ENABLE_MONITORING": "${monitoring.enabled}",
          "METRIC_REPORTERS": "${kafka.metrics.reporters}",
          "BROKER_HEAP_SIZE": "${broker_max_heap_size}",
          "BROKER_JAVA_OPTS": "${broker_java_opts}",
          "BROKER_SSL_ENABLED" : "${ssl_enabled}",
          "KERBEROS_AUTH_ENABLED" : "${kerberos.auth.enable}",
          "SECURITY_INTER_BROKER_PROTOCOL" : "${security.inter.broker.protocol}",
          "AUTHENTICATE_ZOOKEEPER_CONNECTION": "${authenticate.zookeeper.connection}",
          "SUPER_USERS": "${super.users}"
        }
      },
      "stopRunner" : {
        "timeout" : "30000"
      },
      "kerberosPrincipals" : [
        {
          "name" : "KAFKA_PRINCIPAL",
          "primary" : "kafka",
          "instance" : "${host}"
        }
      ],
      "sslServer": {
        "keyIdentifier" : "kafka",
        "enabledConfigName" : "ssl_enabled",
        "keystoreLocationConfigName" : "ssl.keystore.location",
        "keystorePasswordConfigName" : "ssl.keystore.password.generator",
        "keystorePasswordCredentialProviderCompatible" : false,
        "keystorePasswordScriptBased" : true,
        "keyPasswordOptionality" : "required",
        "keystoreKeyPasswordConfigName" : "ssl.key.password.generator",
        "keystoreKeyPasswordScriptBased" : true
      },
      "sslClient": {
        "truststoreLocationConfigName" : "ssl.truststore.location",
        "truststorePasswordConfigName" : "ssl.truststore.password.generator",
        "truststorePasswordScriptBased" : true
      },
      "logging": {
        "dir": "/var/log/kafka",
        "filename": "kafka-broker-${host}.log",
        "modifiable": true,
        "configName": "kafka.log4j.dir",
        "loggingType": "log4j"
      },
      "configWriter": {
        "generators": [
          {
            "filename": "kafka.properties",
            "configFormat": "properties",
            "excludedParams": [
              "ssl_port",
              "jmx_port",
              "security.inter.broker.protocol",
              "monitoring.enabled",
              "kafka.metrics.reporters",
              "zookeeper.chroot",
              "authenticate.zookeeper.connection",
              "broker_max_heap_size",
              "broker_java_opts",
              "ssl_enabled",
              "ssl_port",
              "ssl_server_keystore_location",
              "ssl_client_truststore_location",
              "ssl_server_keystore_password",
              "ssl_server_keystore_keypassword",
              "ssl_client_truststore_password",
              "ssl.client.auth",
              "super.users"
            ],
            "additionalConfigs" : [
              {
                "key" : "#zookeeper.connect",
                "value" : "{{QUORUM}}"
              },
              {
                "key" : "#kafka.metrics.reporters",
                "value" : "{{METRIC_REPORTERS}}"
              },
              {
                "key" : "#security.inter.broker.protocol",
                "value" : "{{SECURITY_INTER_BROKER_PROTOCOL}}"
              },
              {
                "key" : "#ssl.configs",
                "value" : "{{SSL_CONFIGS}}"
              },
              {
                "key" : "#listeners",
                "value" : "{{LISTENERS}}"
              },
              {
                "key": "broker.id.generation.enable",
                "value": "false"
              },
              {
                "key" : "sasl.kerberos.service.name",
                "value" : "kafka"
              }
            ]
          },
          {
            "filename": "ssl.properties",
            "configFormat": "properties",
            "includedParams": [
              "ssl_server_keystore_location",
              "ssl_server_keystore_password",
              "ssl_server_keystore_keypassword",
              "ssl_client_truststore_location",
              "ssl_client_truststore_password",
              "ssl.client.auth"
            ],
            "additionalConfigs" : [
              {
                "key" : "ssl.protocol",
                "value" : "TLS"
              },{
                "key" : "ssl.enabled.protocols",
                "value" : "TLSv1.2,TLSv1.1,TLSv1"
              },{
                "key" : "ssl.keystore.type",
                "value" : "JKS"
              },{
                "key" : "ssl.truststore.type",
                "value" : "JKS"
              }
            ]
          },
          {
            "filename": "kafka-monitoring.properties",
            "configFormat": "properties",
            "includedParams": [
              "monitoring.enabled",
              "kafka.http.metrics.host",
              "kafka.http.metrics.port",
              "broker.id"
            ]
          }
        ]
      },
      "parameters": [
        {
          "name": "broker.id",
          "label": "Broker ID",
          "description": "ID uniquely identifying each broker. Never set this property at the group level; it should always be overridden on instance level.",
          "type": "string"
        },
        {
          "name": "advertised.host.name",
          "label": "Advertised Host",
          "description": "If set, this is the hostname given out to producers, consumers, and other brokers to use in establishing connections. Never set this property at the group level; it should always be overriden on instance level.",
          "type": "string"
        },
        {
          "name": "advertised.port",
          "label": "Advertised Port",
          "description": "The port to give out to producers, consumers, and other brokers to use in establishing connections. This only needs to be set if this port is different from the port the server should bind to.",
          "type": "port"
        },
        {
          "name": "port",
          "label": "TCP Port",
          "description": "Kafka broker port.",
          "type": "port",
          "default": 9092,
          "configurableInWizard": true
        },
        {
          "name": "jmx_port",
          "label": "JMX Port",
          "description": "Port for JMX.",
          "type": "port",
          "default": 9393
        },
        {
          "name": "ssl_port",
          "label": "TLS/SSL Port",
          "description": "Kafka broker secure port.",
          "type": "port",
          "default": 9093,
          "configurableInWizard": true
        },
        {
          "name": "ssl.client.auth",
          "label": "SSL Client Authentication",
          "description": "Client authentication mode for SSL connections. Default is none, could be set to \"required\", i.e., client authentication is required or to \"requested\", i.e., client authentication is requested and client without certificates can still connect.",
          "type": "string_enum",
          "validValues" : [ "none", "required", "requested" ],
          "default": "none"
        },
        {
          "name": "security.inter.broker.protocol",
          "label": "Inter Broker Protocol",
          "description": "Protocol to be used for inter-broker communication. INFERRED will use the same protocol configured for external clients.",
          "type": "string_enum",
          "validValues" : [ "INFERRED", "PLAINTEXT", "SSL", "SASL_PLAINTEXT", "SASL_SSL" ],
          "default": "INFERRED"
        },
        {
          "name": "authenticate.zookeeper.connection",
          "label": "Authenticate Zookeeper Connection",
          "description": "Authenticate a SASL connection with zookeeper, if Kerberos authentication is enabled. It also allows a broker to set SASL ACL on zookeeper nodes which locks these nodes down so that only kafka broker can modify.",
          "type": "boolean",
          "default": true
        },
        {
          "name": "super.users",
          "label": "Super users",
          "description": "Users who are allowed to perform any action on the Kafka cluster.",
          "type" : "string_array",
          "separator" : ";",
          "default": ["kafka"]
        },
        {
          "name": "log.dirs",
          "label": "Data Directories",
          "description": "A list of one or more directories in which Kafka data is stored. Each new partition created is placed in the directory that currently has the fewest partitions. Each directory should be on its own separate drive.",
          "type": "path_array",
          "default": [
            "/var/local/kafka/data"
          ],
          "pathType": "localDataDir",
          "required": "true",
          "minLength": 1,
          "configurableInWizard": true
        },
        {
          "name": "log.segment.bytes",
          "label": "Segment File Size",
          "description": "The log for a topic partition is stored as a directory of segment files. This setting controls the size to which a segment file can grow before a new segment is rolled over in the log. This value should be larger than message.max.bytes.",
          "type": "long",
          "default": 1073741824,
          "min": 14,
          "max": 2147483647,
          "softMin": 1024,
          "unit": "bytes"
        },
        {
          "name": "log.retention.hours",
          "label": "Data Retention Hours",
          "description": "The maximum time before a new log segment is rolled out (in hours). Secondary to the log.retention.ms property. The special value of -1 is interpreted as unlimited. This property is deprecated in Kafka 1.4.0. Use log.retention.ms.",
          "type": "long",
          "default": 168,
          "min": -1,
          "max": 2147483647,
          "unit": "hours"
        },
        {
          "name": "log.retention.ms",
          "label": "Data Retention Time",
          "description": "The maximum time before a new log segment is rolled out. If both log.retention.ms and log.retention.bytes are set, a segment is deleted when either limit is exceeded. The special value of -1 is interpreted as unlimited. This property is used in Kafka 1.4.0 and later in place of log.retention.hours.",
          "type": "long",
          "min": -1,
          "unit": "milliseconds"
        },
        {
          "name": "log.retention.bytes",
          "label": "Data Retention Size",
          "description": "The amount of data to retain in the log for each topic-partition. This is the limit per partition: multiply by the number of partitions to get the total data retained for the topic. The special value of -1 is interpreted as unlimited. If both log.retention.ms and log.retention.bytes are set, a segment is deleted when either limit is exceeded.",
          "type": "long",
          "default": -1,
          "min": -1,
          "unit": "bytes"
        },
        {
          "name": "log.retention.check.interval.ms",
          "label": "Data Retention Check Interval",
          "description": "The frequency, in milliseconds, that the log cleaner checks whether any log segment is eligible for deletion, per retention policies.",
          "type": "long",
          "default": 300000,
          "min": 1,
          "unit": "milliseconds"
        },
        {
          "name": "log.roll.hours",
          "label": "Data Log Roll Hours",
          "description": "The maximum time before a new log segment is rolled out (in hours). This property is deprecated in Cloudera Kafka 1.4.0; use log.roll.ms.",
          "type": "long",
          "default": 168,
          "min": 1,
          "max": 2147483647,
          "unit": "hours"
        },
        {
          "name": "log.roll.ms",
          "label": "Data Log Roll Time",
          "description": "The maximum time before a new log segment is rolled out. This property is used in Cloudera Kafka 1.4.0 and later in place of log.roll.hours.",
          "type": "long",
          "min": 1,
          "unit": "milliseconds"
        },
        {
          "name": "num.io.threads",
          "label": "Number of I/O Threads",
          "description": "The number of I/O threads that the server uses for executing requests. You should have at least as many threads as you have disks.",
          "type": "long",
          "default": 8,
          "min": 1,
          "max": 2147483647,
          "softMax": 100
        },
        {
          "name": "max.connections.per.ip",
          "label": "Maximum Connections per IP Address",
          "description": "Maximum number of connections allowed from each IP address.",
          "type": "long",
          "min": 1,
          "max": 2147483647
        },
        {
          "name": "kafka.http.metrics.host",
          "label": "HTTP Metric Report Host",
          "description": "Host the HTTP metric reporter binds to.",
          "type": "string",
          "default": "0.0.0.0"
        },
        {
          "name": "kafka.http.metrics.port",
          "label": "HTTP Metric Report Port",
          "description": "Port the HTTP metric reporter listens on.",
          "type": "port",
          "default": 24042
        },
        {
          "name": "broker_max_heap_size",
          "label": "Java Heap Size of Broker",
          "description": "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in megabytes. Kafka does not generally require setting large heap sizes. It is better to let the file system cache utilize the available memory.",
          "type": "memory",
          "default": 1024,
          "min": 50,
          "softMin": 256,
          "softMax": 16384,
          "unit": "megabytes",
          "scaleFactor" : 1.3
        },
        {
          "name": "broker_java_opts",
          "label": "Additional Broker Java Options",
          "description": "These arguments are passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags are passed here.",
          "type": "string",
          "default": "-server -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true"
        }
      ]
    },
    {
      "name": "KAFKA_MIRROR_MAKER",
      "label": "Kafka MirrorMaker",
      "pluralLabel": "Kafka MirrorMakers",
      "jvmBased": true,
      "startRunner": {
        "program": "scripts/mirrormaker_control.sh",
        "args": [
          "start"
        ],
        "environmentVariables": {
          "CHROOT": "${zookeeper.chroot}",
          "NO_DATA_LOSS": "${no.data.loss}",
          "WHITELIST": "${whitelist}",
          "BLACKLIST": "${blacklist}",
          "NUM_PRODUCERS": "${num.producers}",
          "NUM_STREAMS": "${num.streams}",
          "QUEUE_SIZE": "${queue.size}",
          "QUEUE_BYTE_SIZE": "${queue.byte.size}",
          "JMX_PORT": "${jmx_port}",
          "MM_HEAP_SIZE": "${mirror_maker_max_heap_size}",
          "MM_JAVA_OPTS": "${mirror_maker_java_opts}",
          "ABORT_ON_SEND_FAILURE": "${abort.on.send.failure}",
          "OFFSET_COMMIT_INTERVAL_MS": "${offset.commit.interval.ms}",
          "CONSUMER_REBALANCE_LISTENER": "${consumer.rebalance.listener}",
          "CONSUMER_REBALANCE_LISTENER_ARGS": "${consumer.rebalance.listener.args}",
          "MESSAGE_HANDLER": "${message.handler}",
          "MESSAGE_HANDLER_ARGS": "${message.handler.args}",
          "SOURCE_SECURITY_PROTOCOL": "${source.security.protocol}",
          "DESTINATION_SECURITY_PROTOCOL": "${destination.security.protocol}",
          "SOURCE_SSL_CLIENT_AUTH": "${source.ssl.client.auth}",
          "DESTINATION_SSL_CLIENT_AUTH": "${destination.ssl.client.auth}"
        }
      },
      "logging": {
        "dir": "/var/log/kafka",
        "filename": "kafka-mirrormaker-${host}.log",
        "modifiable": true,
        "configName": "kafka_mirrormaker.log4j.dir",
        "loggingType": "log4j"
      },
      "topology": {
        "minInstances": "0"
      },
      "sslServer": {
        "keyIdentifier" : "kafka_mirror_maker",
        "enabledConfigName" : "ssl_enabled",
        "keystoreLocationConfigName" : "ssl.keystore.location",
        "keystorePasswordConfigName" : "ssl.keystore.password.generator",
        "keystorePasswordCredentialProviderCompatible" : false,
        "keystorePasswordScriptBased" : true,
        "keyPasswordOptionality" : "required",
        "keystoreKeyPasswordConfigName" : "ssl.key.password.generator",
        "keystoreKeyPasswordScriptBased" : true
      },
      "sslClient": {
        "truststoreLocationConfigName" : "ssl.truststore.location",
        "truststorePasswordConfigName" : "ssl.truststore.password.generator",
        "truststorePasswordScriptBased" : true
      },
      "kerberosPrincipals" : [
        {
          "name" : "KAFKA_MIRROR_MAKER_PRINCIPAL",
          "primary" : "kafka_mirror_maker",
          "instance" : "${host}"
        }
      ],
      "configWriter": {
        "generators": [
          {
            "filename": "mirror_maker_producers.properties",
            "configFormat": "properties",
            "includedParams": [
              "bootstrap.servers",
              "linger.ms",
              "compression.type",
              "batch.size",
              "buffer.memory"
            ],
            "additionalConfigs" : [
              {
                "key" : "request.timeout.ms",
                "value" : "${producer.request.timeout.ms}"
              },
              {
                "key" : "#ssl.configs",
                "value" : "{{SSL_CONFIGS}}"
              },
              {
                "key" : "security.protocol",
                "value" : "${destination.security.protocol}"
              },
              {
                "key" : "sasl.kerberos.service.name",
                "value" : "kafka"
              }
            ]
          },
          {
            "filename": "mirror_maker_consumers.properties",
            "configFormat": "properties",
            "includedParams": [
              "group.id",
              "session.timeout.ms",
              "fetch.min.bytes"
            ],
            "additionalConfigs" : [
              {
                "key" : "bootstrap.servers",
                "value" : "${source.bootstrap.servers}"
              },
              {
                "key" : "request.timeout.ms",
                "value" : "${consumer.request.timeout.ms}"
              },
              {
                "key" : "#ssl.configs",
                "value" : "{{SSL_CONFIGS}}"
              },
              {
                "key" : "security.protocol",
                "value" : "${source.security.protocol}"
              },
              {
                "key" : "sasl.kerberos.service.name",
                "value" : "kafka"
              }
            ]
          },
          {
            "filename": "ssl_server.properties",
            "configFormat": "properties",
            "includedParams": [
              "ssl_server_keystore_location",
              "ssl_server_keystore_password",
              "ssl_server_keystore_keypassword"
            ]
          },
          {
            "filename": "ssl_client.properties",
            "configFormat": "properties",
            "includedParams": [
              "ssl_client_truststore_location",
              "ssl_client_truststore_password"
            ]
          }
        ]
      },
      "parameters": [
        {
          "name": "group.id",
          "label": "Consumer Group ID",
          "description": "Name of the consumer group used by MirrorMaker. When multiple role instances are configured with the same topics and same group ID, the role instances load-balance replication for the topics. When multiple role instances are configured with the same topics but different group ID, each role instance replicates all the events for those topics - this can be used to replicate the source cluster into multiple destination clusters.",
          "type": "string",
          "default": "cloudera_mirrormaker"
        },
        {
          "name": "bootstrap.servers",
          "label": "Destination Broker List",
          "description": "List of brokers on destination cluster. This should be more than one, for high availability, but there's no need to list all brokers.",
          "type": "string",
          "required": "true",
          "configurableInWizard": true
        },
        {
          "name": "source.bootstrap.servers",
          "label": "Source Broker List",
          "description": "List of brokers on source cluster. This should be more than one, for high availability, but there's no need to list all brokers.",
          "type": "string",
          "required": "true",
          "configurableInWizard": true
        },
        {
          "name": "no.data.loss",
          "label": "Avoid Data Loss",
          "description": "Run with MirrorMaker settings that eliminate potential loss of data. This impacts performance, but is highly recommended. WARNING: Does not work with Kafka 2.0 or later.",
          "type": "boolean",
          "default": true
        },
        {
          "name": "whitelist",
          "label": "Topic Whitelist",
          "description": "Regular expression that represents a set of topics to mirror. Note that whitelist and blacklist parameters are mutually exclusive. If both are defined, only the whilelist is used.",
          "type": "string",
          "default": "",
          "configurableInWizard": true
        },
        {
          "name": "blacklist",
          "label": "Topic Blacklist",
          "description": "Regular expression that represents a set of topics to avoid mirroring. Note that whitelist and blacklist parameters are mutually exclusive. If both are defined, only the whilelist is used. WARNING: Does not work with Kafka 2.0 or later.",
          "type": "string",
          "default": "",
          "configurableInWizard": true
        },
        {
          "name": "num.producers",
          "label": "Number of Producers",
          "description": "Number of producer instances. WARNING: Does not work with Kafka 2.0 or later.",
          "type": "long",
          "default": 1,
          "min": 1
        },
        {
          "name": "num.streams",
          "label": "Number of Consumer Threads",
          "description": "Number of consumer threads.",
          "type": "long",
          "default": 1,
          "min": 1
        },
        {
          "name": "queue.size",
          "label": "Message Queue Size",
          "description": "Number of messages that are buffered between producer and consumer. WARNING: Does not work with Kafka 2.0 or later.",
          "type": "long",
          "default": 10000,
          "min": 1
        },
        {
          "name": "queue.byte.size",
          "label": "Queue Size",
          "description": "Maximum number of bytes that can be buffered between producer and consumer. WARNING: Does not work with Kafka 2.0 or later.",
          "type": "long",
          "default": 100000000,
          "unit": "bytes"
        },
        {
          "name": "jmx_port",
          "label": "JMX Port",
          "description": "Port for JMX.",
          "type": "port",
          "default": 9394
        },
        {
          "name": "abort.on.send.failure",
          "label": "Abort on Send Failure",
          "description": "Stop the entire mirror maker when a send failure occurs.",
          "type": "boolean",
          "default": true
        },
        {
          "name": "offset.commit.interval.ms",
          "label": "Offset Commit Interval",
          "description": "Offset commit interval in milliseconds.",
          "type": "long",
          "default": 60000,
          "min": 1
        },
        {
          "name": "linger.ms",
          "label": "Producer Linger Time",
          "description": "The upper bound on the delay for batching. Once the producer gets a batch.size worth of records for a partition it will be sent immediately regardless of this setting. However if fewer than this many bytes accumulated for this partition the producer will 'linger' for the specified time waiting for more records to show up. Only respected by Kafka 2.0 or later.",
          "type": "long",
          "default": 0,
          "min": 0,
          "unit": "milliseconds"
        },
        {
          "name": "batch.size",
          "label": "Producer Batch Size",
          "description": "This configuration controls the batch size in bytes. The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. Only respected by Kafka 2.0 or later.",
          "type": "long",
          "default": 16384,
          "min": 1,
          "max": 2147483647,
          "unit": "bytes"
        },
        {
          "name": "buffer.memory",
          "label": "Producer Buffer Memory",
          "description": "The total bytes of memory the producer can use to buffer records waiting to be sent to the server. Only respected by Kafka 2.0 or later.",
          "type": "long",
          "default": 33554432,
          "min": 1,
          "unit": "bytes"
        },
        {
          "name": "compression.type",
          "label": "Producer Compression Type",
          "description": "The compression type for all data generated by the producer. Only respected by Kafka 2.0 or later.",
          "type": "string_enum",
          "validValues" : [ "none", "snappy", "gzip" ],
          "default": "none"
        },
        {
          "name": "producer.request.timeout.ms",
          "label": "Producer Request Timeout",
          "description": "The maximum amount of time the producer will wait for the response of a request. If the response is not received before the timeout elapses the producer will resend the request if necessary or fail the request if retries are exhausted. Only respected by Kafka 2.0 or later.",
          "type": "long",
          "default": 30000,
          "min": 1,
          "max": 2147483647,
          "unit": "milliseconds"
        },
        {
          "name": "consumer.request.timeout.ms",
          "label": "Consumer Request Timeout",
          "description": "The maximum amount of time the consumer will wait for the response of a request. If the response is not received before the timeout elapses the consumer will resend the request if necessary or fail the request if retries are exhausted. Only respected by Kafka 2.0 or later.",
          "type": "long",
          "default": 40000,
          "min": 1,
          "max": 2147483647,
          "unit": "milliseconds"
        },
        {
          "name": "session.timeout.ms",
          "label": "Consumer Session Timeout",
          "description": "The timeout used to detect failures when using Kafka's group management facilities. When a consumer's heartbeat is not received within the session timeout, the broker will mark the consumer as failed and rebalance the group. Note that the value must be in the allowable range as configured in the broker by group.min.session.timeout.ms and group.max.session.timeout.ms. Only respected by Kafka 2.0 or later.",
          "type": "long",
          "default": 30000,
          "min": 1,
          "max": 2147483647,
          "unit": "milliseconds"
        },
        {
          "name": "fetch.min.bytes",
          "label": "Consumer Minimum Fetch Size",
          "description": "The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency.",
          "type": "long",
          "default": 1,
          "min": 1,
          "max": 2147483647,
          "unit": "bytes"
        },
        {
          "name": "consumer.rebalance.listener",
          "label": "MirrorMaker Consumer Rebalance Listener",
          "description": "A consumer rebalance listener of type ConsumerRebalanceListener to be invoked when MirrorMaker's consumer rebalances.",
          "type": "string",
          "default": ""
        },
        {
          "name": "consumer.rebalance.listener.args",
          "label": "MirrorMaker Consumer Rebalance Listener Arguments",
          "description": "Arguments used by MirrorMaker consumer rebalance listener.",
          "type": "string",
          "default": ""
        },
        {
          "name": "message.handler",
          "label": "MirrorMaker Message Handler",
          "description": "A MirrorMaker message handler of type MirrorMakerMessageHandler that will process every record in-between producer and consumer.",
          "type": "string",
          "default": ""
        },
        {
          "name": "message.handler.args",
          "label": "MirrorMaker Message Handler Arguments",
          "description": "Arguments used by MirrorMaker message handler.",
          "type": "string",
          "default": ""
        },
        {
          "name": "source.security.protocol",
          "label": "Source Kafka Cluster's Security Protocol",
          "description": "Protocol to be used for communication with source kafka cluster.",
          "type": "string_enum",
          "validValues" : [ "PLAINTEXT", "SSL", "SASL_PLAINTEXT", "SASL_SSL" ],
          "default": "PLAINTEXT"
        },
        {
          "name": "destination.security.protocol",
          "label": "Destination Kafka Cluster's Security Protocol",
          "description": "Protocol to be used for communication with destination kafka cluster.",
          "type": "string_enum",
          "validValues" : [ "PLAINTEXT", "SSL", "SASL_PLAINTEXT", "SASL_SSL" ],
          "default": "PLAINTEXT"
        },
        {
          "name": "source.ssl.client.auth",
          "label": "Source Kafka Cluster's Client Auth",
          "description": "Only required if source Kafka cluster requires client authentication.",
          "type": "boolean",
          "default": false
        },
        {
          "name": "destination.ssl.client.auth",
          "label": "Destination Kafka Cluster's Client Auth",
          "description": "Only required if destination Kafka cluster requires client authentication.",
          "type": "boolean",
          "default": false
        },
        {
          "name": "mirror_maker_max_heap_size",
          "label": "Java Heap Size of MirrorMaker",
          "description": "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in megabytes.",
          "type": "memory",
          "default": 1024,
          "min": 50,
          "softMin": 256,
          "softMax": 16384,
          "unit": "megabytes",
          "scaleFactor" : 1.3
        },
        {
          "name": "mirror_maker_java_opts",
          "label": "Additional MirrorMaker Java Options",
          "description": "These arguments are passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags are passed here.",
          "type": "string",
          "default": "-server -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true"
        }
      ]
    }
  ],
  "gateway" : {
    "alternatives" : {
      "name" : "kafka-conf",
      "priority" : 50,
      "linkRoot" : "/etc/kafka"
    },
    "scriptRunner" : {
      "program" : "scripts/deploy_client_config.sh",
      "environmentVariables" : {
        "CHROOT" : "${zookeeper.chroot}"
      }
    },
    "logging" : {
      "configFilename" : "kafka-conf/tools-log4j.properties",
      "loggingType" : "log4j"
    },
    "configWriter" : {
      "generators" : [
        {
          "filename" : "kafka-conf/kafka-client.conf",
          "configFormat" : "properties",
          "includedParams": [
          ],
          "additionalConfigs" : [
            {
              "key" : "#zookeeper.connect",
              "value" : "{{QUORUM}}"
            }
          ]
        }
      ]
    }
  },
  "rollingRestart": {
    "nonWorkerSteps": [
      {
        "roleName": "KAFKA_BROKER",
        "bringUpCommands": [ "Start" ],
        "bringDownCommands": [ "Stop" ]
      },
      {
        "roleName": "KAFKA_MIRROR_MAKER",
        "bringUpCommands": [ "Start" ],
        "bringDownCommands": [ "Stop" ]
      }
    ]
  }
}
